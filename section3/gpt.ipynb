{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers torch\n",
    "# Author: Chittenden, R. H. (Russell Henry)\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_text(url):\n",
    "    response = urlopen(url)\n",
    "    raw_text = response.read().decode('utf-8')\n",
    "    return raw_text\n",
    "\n",
    "# On Digestive Proteolysis\n",
    "url1 = \"https://www.gutenberg.org/cache/epub/47938/pg47938.txt\"  \n",
    "raw_text1 = download_text(url1)\n",
    "\n",
    "# The nutrition of man\n",
    "url2 = \"https://www.gutenberg.org/cache/epub/69439/pg69439.txt\"  \n",
    "raw_text2 = download_text(url2)\n",
    "\n",
    "# Physiological economy in nutrition, with special reference to the minimal proteid requirement of the healthy man an \n",
    "# experimental study\n",
    "url3 = \"https://www.gutenberg.org/cache/epub/68830/pg68830.txt\"  \n",
    "raw_text3 = download_text(url3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = raw_text1 + raw_text2 + raw_text3\n",
    "# combined_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onkars/Documents/PSYC681/Problem Set/nlp1_rit_course/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/onkars/Documents/PSYC681/Problem Set/nlp1_rit_course/myenv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "encodings = tokenizer(combined_dataset, return_tensors='pt', max_length=1024, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings['input_ids']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.encodings.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encodings[idx]\n",
    "\n",
    "dataset = BookDataset(encodings)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# model.train()\n",
    "num_epochs = 5\n",
    "gradient_accumulation_steps = 7\n",
    "print(len(dataloader))\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     loop = tqdm(dataloader, leave=True)\n",
    "#     for batch in loop:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(batch, labels=batch)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # Update the progress bar\n",
    "#         loop.set_description(f'Epoch {epoch}')\n",
    "#         loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "total_training_steps = len(dataloader) * num_epochs\n",
    "warmup_steps = int(0.1 * total_training_steps)  # 10% of the training steps\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=warmup_steps, \n",
    "                                            num_training_steps=total_training_steps)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_gpt/tokenizer_config.json',\n",
       " './fine_tuned_gpt/special_tokens_map.json',\n",
       " './fine_tuned_gpt/vocab.json',\n",
       " './fine_tuned_gpt/merges.txt',\n",
       " './fine_tuned_gpt/added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop (simplified example)\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "#         # Gradient accumulation step (if needed)\n",
    "#         if gradient_accumulation_steps > 1:\n",
    "#             if (step + 1) % gradient_accumulation_steps == 0:\n",
    "#                 optimizer.step()\n",
    "#                 scheduler.step()\n",
    "#                 optimizer.zero_grad()\n",
    "#         else:\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "model.save_pretrained('./fine_tuned_gpt')\n",
    "tokenizer.save_pretrained('./fine_tuned_gpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='./fine_tuned_gpt', tokenizer='./fine_tuned_gpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 (Starting Word: 'the'):\n",
      " [{'generated_text': 'the-day-book\">How To Survive The Yearbook</div></article>\\n\\n1) You won\\'t be\n",
      "able to catch the yearbook, but you\\'ll have access to the internet.\\n\\nAs we saw several other\n",
      "days'}]\n",
      "\n",
      "Sample 2 (Starting Word: 'he'):\n",
      " [{'generated_text': 'he-Shiite Shiites, among others, who have the most power. However, when the\n",
      "Shiites were attacked by the Iranian army, they went to a village and found the bodies of a large\n",
      "number of Shiites. After a time'}]\n",
      "\n",
      "Sample 3 (Starting Word: 'hence'):\n",
      " [{'generated_text': 'hence, a former leader of the Democratic Party in South Africa\\'s Legislative\n",
      "Assembly (LCA), said.\\n\\n\"I am so tired of saying it, but that\\'s not the way that our country\\'s\n",
      "future was supposed to be held in'}]\n"
     ]
    }
   ],
   "source": [
    "sample1 = generator(\"the\", max_length=50, num_return_sequences=1)\n",
    "sample2 = generator(\"he\", max_length=50, num_return_sequences=1)\n",
    "sample3 = generator(\"hence\", max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Print generated samples\n",
    "print(\"Sample 1 (Starting Word: 'the'):\\n\", textwrap.fill(str(sample1), width=100))\n",
    "print(\"\\nSample 2 (Starting Word: 'he'):\\n\", textwrap.fill(str(sample2), width=100))\n",
    "print(\"\\nSample 3 (Starting Word: 'hence'):\\n\", textwrap.fill(str(sample3), width=100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 (Specific Word: 'acid'):\n",
      " [{'generated_text': 'acid has been found in all 653 cases of manganese skin lesions caused by this\n",
      "chemical type, and some studies have suggested that these lesions can lead to the development of\n",
      "some autoimmune thyroid diseases. In a study led by the University of California,'}]\n",
      "\n",
      "Sample 2 (Specific Word: 'proteid'):\n",
      " [{'generated_text': 'proteidosis.\\n\\n3-HTR-D3-HIP3-4-INV-H-3,INR-3-PIPE-3-p3+ (1â€“42 mM, 0.'}]\n",
      "\n",
      "Sample 3 (Specific Word: 'peptone'):\n",
      " [{'generated_text': 'peptone (7), the most prominent compound that has a role in opioid addiction\n",
      "and related behaviors. These substances can increase the risk of addiction, which is a symptom of\n",
      "increased opioid use. If users are taking them with a high dose of opioids'}]\n"
     ]
    }
   ],
   "source": [
    "sample1 = generator(\"acid\", max_length=50, num_return_sequences=1)\n",
    "sample2 = generator(\"proteid\", max_length=50, num_return_sequences=1)\n",
    "sample3 = generator(\"peptone\", max_length=50, num_return_sequences=1)\n",
    "# Print generated samples\n",
    "print(\"Sample 1 (Specific Word: 'acid'):\\n\", textwrap.fill(str(sample1), width=100))\n",
    "print(\"\\nSample 2 (Specific Word: 'proteid'):\\n\", textwrap.fill(str(sample2), width=100))\n",
    "print(\"\\nSample 3 (Specific Word: 'peptone'):\\n\", textwrap.fill(str(sample3), width=100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 (Function Word: 'of'):\n",
      " [{'generated_text': 'of on your side. It takes only two to three points for me to have a good\n",
      "understanding of what a certain group of characters are doing and what they should be\n",
      "thinking.\\n\\nGrowth as such has been a big part of my play,'}]\n",
      "\n",
      "Sample 2 (Function Word: 'is'):\n",
      " [{'generated_text': 'is a place where the first responders can learn about what has happened.\\n\\nIt\n",
      "has led to questions about whether police can properly handle complaints. One has the potential to\n",
      "undermine community trust due to possible bias for officers trying to intervene.\\n\\nAnd'}]\n",
      "\n",
      "Sample 3 (Function Word: 'by'):\n",
      " [{'generated_text': 'by-laws that are passed in the community,\" said Richard L. Clements, the\n",
      "director of the community program at the South Carolina Legal Education Association. \"It doesn\\'t\n",
      "matter who is elected, but in general, the more that we have the'}]\n"
     ]
    }
   ],
   "source": [
    "sample1 = generator(\"of\", max_length=50, num_return_sequences=1)\n",
    "sample2 = generator(\"is\", max_length=50, num_return_sequences=1)\n",
    "sample3 = generator(\"by\", max_length=50, num_return_sequences=1)\n",
    "# Print generated samples\n",
    "print(\"Sample 1 (Function Word: 'of'):\\n\", textwrap.fill(str(sample1), width=100))\n",
    "print(\"\\nSample 2 (Function Word: 'is'):\\n\", textwrap.fill(str(sample2), width=100))\n",
    "print(\"\\nSample 3 (Function Word: 'by'):\\n\", textwrap.fill(str(sample3), width=100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1 (Content Word: 'may'):\n",
      " [{'generated_text': \"may, this will help us in the long run to not have to deal with this situation\n",
      "because in the long term, if we want to fight effectively we have to fight the people. Right now,\n",
      "I'd say if we win, we can get\"}]\n",
      "\n",
      "Sample 2 (Content Word: 'action'):\n",
      " [{'generated_text': 'action of the system by the government; and this, moreover, was made a matter\n",
      "of policy by the government in order to further the public interest. That the government had\n",
      "provided for the provision of services in particular, and therefore that the government and its'}]\n",
      "\n",
      "Sample 3 (Content Word: 'products'):\n",
      " [{'generated_text': 'products.\\n\\nThe study also provides new insight into the process of gene\n",
      "editing the genomes of plants and animals, and may help scientists understand how the immune system\n",
      "may react to genetically modified organisms (GM), as well as what might happen to wild\n",
      "populations'}]\n"
     ]
    }
   ],
   "source": [
    "sample1 = generator(\"may\", max_length=50, num_return_sequences=1)\n",
    "sample2 = generator(\"action\", max_length=50, num_return_sequences=1)\n",
    "sample3 = generator(\"products\", max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Print generated samples\n",
    "print(\"Sample 1 (Content Word: 'may'):\\n\", textwrap.fill(str(sample1), width=100))\n",
    "print(\"\\nSample 2 (Content Word: 'action'):\\n\", textwrap.fill(str(sample2), width=100))\n",
    "print(\"\\nSample 3 (Content Word: 'products'):\\n\", textwrap.fill(str(sample3), width=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
